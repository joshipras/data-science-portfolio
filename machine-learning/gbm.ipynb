{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " $('#toggleButton').val('Show Code')\n",
       " } else {\n",
       " $('div.input').show();\n",
       " $('#toggleButton').val('Show Code')\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id = \"toggleButton\" value=\"Hide Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " $('#toggleButton').val('Show Code')\n",
    " } else {\n",
    " $('div.input').show();\n",
    " $('#toggleButton').val('Show Code')\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id = \"toggleButton\" value=\"Hide Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import HTML, Image\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score,classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc as sk_auc\n",
    "import pydotplus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Gradient Boosting Trees using Python </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Introduction </h2>\n",
    "<br>\n",
    "\n",
    "<li> Boosting algorithms are a family of machine learning algorithms that combine \"weak learners\" to form a \"strong learner\". A weak learner is any machine learning algorithm that has better accuracy than simply guessing, but only performs well on a subset of the data. Boosting works by using these to solve subsections of the problem, by peeling them away so future boosting iterations can solve the remaining sections.\n",
    "\n",
    "<li> For instance, an algorithm classifying animals at a zoo only classifies zebras correctly, most of the time, and simply guesses for other animals. Boosting involves using the model for zebras while other weak learners to focus on the remaining animals.\n",
    "    \n",
    "<li> In the case of Gradient Boosting Trees, Decision Trees are the weak learners.  \n",
    "\n",
    "<br>\n",
    "<h2> Gradient Boosting Regression </h2>\n",
    "<br>    \n",
    "For Regression, the algorithm works as follows:\n",
    "<ol>\n",
    "<li> We predict a 0 for each value and calculate the error over the dataset.\n",
    "<li> The tree begins with the initial split into 2 groups. Average difference of each value and the initial prediction of 0. Each possible split of the tree is examined. I.e. the data is split around each value in the dataset. \n",
    "<li> The split is selected so that values closest to each other are clumped together (i.e Sum of Squared errors of Split Value and each datapoint) is minimized.\n",
    "<li> The average error in the 2 subsets is added to all values in the group.\n",
    "<li> We Split each dataset again into 2 groups and repeat the process until a certain depth is reached.\n",
    "</ol>    \n",
    "    \n",
    "    \n",
    "The dataset used here is the Online News Popularity Dataset from <a href = http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity>UCI Machine Learning Repository </a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import the dataset\n",
    "We inspect the distribution of variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  39644.000000    39644.000000      39644.000000     39644.000000   \n",
       "mean     354.530471       10.398749        546.514731         0.548216   \n",
       "std      214.163767        2.114037        471.107508         3.520708   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.470870   \n",
       "50%      339.000000       10.000000        409.000000         0.539226   \n",
       "75%      542.000000       12.000000        716.000000         0.608696   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      39644.000000              39644.000000  39644.000000   \n",
       "mean           0.996469                  0.689175     10.883690   \n",
       "std            5.231231                  3.264816     11.332017   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.625739      4.000000   \n",
       "50%            1.000000                  0.690476      8.000000   \n",
       "75%            1.000000                  0.754630     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos  ...  min_positive_polarity  \\\n",
       "count    39644.000000  39644.000000  39644.000000  ...           39644.000000   \n",
       "mean         3.293638      4.544143      1.249874  ...               0.095446   \n",
       "std          3.855141      8.309434      4.107855  ...               0.071315   \n",
       "min          0.000000      0.000000      0.000000  ...               0.000000   \n",
       "25%          1.000000      1.000000      0.000000  ...               0.050000   \n",
       "50%          3.000000      1.000000      0.000000  ...               0.100000   \n",
       "75%          4.000000      4.000000      1.000000  ...               0.100000   \n",
       "max        116.000000    128.000000     91.000000  ...               1.000000   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.756728              -0.259524              -0.521944   \n",
       "std                 0.247786               0.127726               0.290290   \n",
       "min                 0.000000              -1.000000              -1.000000   \n",
       "25%                 0.600000              -0.328383              -0.700000   \n",
       "50%                 0.800000              -0.253333              -0.500000   \n",
       "75%                 1.000000              -0.186905              -0.300000   \n",
       "max                 1.000000               0.000000               0.000000   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "count           39644.000000        39644.000000              39644.000000   \n",
       "mean               -0.107500            0.282353                  0.071425   \n",
       "std                 0.095373            0.324247                  0.265450   \n",
       "min                -1.000000            0.000000                 -1.000000   \n",
       "25%                -0.125000            0.000000                  0.000000   \n",
       "50%                -0.100000            0.150000                  0.000000   \n",
       "75%                -0.050000            0.500000                  0.150000   \n",
       "max                 0.000000            1.000000                  1.000000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity         shares  \n",
       "count            39644.000000                  39644.000000   39644.000000  \n",
       "mean                 0.341843                      0.156064    3395.380184  \n",
       "std                  0.188791                      0.226294   11626.950749  \n",
       "min                  0.000000                      0.000000       1.000000  \n",
       "25%                  0.166667                      0.000000     946.000000  \n",
       "50%                  0.500000                      0.000000    1400.000000  \n",
       "75%                  0.500000                      0.250000    2800.000000  \n",
       "max                  0.500000                      1.000000  843300.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read csv data\n",
    "news_data = pd.read_csv(\"OnlineNewsPopularity.csv\")\n",
    "\n",
    "# Remove white space from column names:\n",
    "news_data.columns = news_data.columns.str.lstrip()\n",
    "\n",
    "# Check data types of the variables\n",
    "display(news_data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Data pre-processing\n",
    "Given the variance of the dependant variable, we convert it to binary and change the problem to one of classification instead of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable popularity based on number of shares:\n",
    "news_data.loc[news_data.shares >= 1400,'popular'] = 1\n",
    "news_data.loc[news_data.shares < 1400,'popular'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target and training features\n",
    "X = news_data.drop(['popular','shares','url'],axis = 1)\n",
    "y = news_data['popular']\n",
    "y = pd.Series(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split into train, test and validation sets\n",
    "\n",
    "We use min-max scaling to normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 123)\n",
    "\n",
    "# Scaling using MinMaxScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "# Split training data into testing and validation\n",
    "\n",
    "X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train and Fit Bradient Boosting Classifier\n",
    "We evaluate performance of the Gradient Boosting Classifier for a a range of learning rates between 0.01 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.01\n",
      "Accuracy score (training): 0.762\n",
      "Accuracy score (validation): 0.669\n",
      "\n",
      "Learning rate:  0.05\n",
      "Accuracy score (training): 0.899\n",
      "Accuracy score (validation): 0.667\n",
      "\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.976\n",
      "Accuracy score (validation): 0.662\n",
      "\n",
      "Learning rate:  0.25\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.652\n",
      "\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.648\n",
      "\n",
      "Learning rate:  0.75\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.635\n",
      "\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 0.627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01,0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=500, learning_rate = learning_rate, max_features=40, max_depth = 6, random_state = 0)\n",
    "    gb.fit(X_train_sub, y_train_sub)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train_sub, y_train_sub)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validation_sub, y_validation_sub)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Model evaluation:\n",
    "Learning rate of 0.1 seems to give the best accuracy on training and validation. Let us review the confusion matrix and classification report with a learning rate of 0.1 on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1970 1285]\n",
      " [1059 2624]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.61      0.63      3255\n",
      "         1.0       0.67      0.71      0.69      3683\n",
      "\n",
      "    accuracy                           0.66      6938\n",
      "   macro avg       0.66      0.66      0.66      6938\n",
      "weighted avg       0.66      0.66      0.66      6938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=500, learning_rate = 0.1, max_features=40, max_depth = 6, random_state = 0)\n",
    "gb.fit(X_train_sub, y_train_sub)\n",
    "predictions = gb.predict(X_validation_sub)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_validation_sub, predictions))\n",
    "print()\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_validation_sub, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also visualize the ROC curve and visualize area under the curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and Area-Under-Curve (AUC)\n",
    "\n",
    "y_pred_proba = gb.predict_proba(X_validation_sub)[::,1]\n",
    "y_scores_gb = gb.decision_function(X_validation_sub)\n",
    "\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_validation_sub, y_scores_gb)\n",
    "\n",
    "roc_auc_gb = sk_auc(fpr_gb, tpr_gb)\n",
    "\n",
    "plt.plot(fpr_gb,tpr_gb,label=\"data 1, auc=\"+str(roc_auc_gb))\n",
    "\n",
    "print(\"Area under ROC curve = {:0.2f}\".format(roc_auc_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vizualize one of the trees to understand a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize one of the trees\n",
    "sub_tree_42 = gb.estimators_[42, 0]\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    sub_tree_42,\n",
    "    out_file=None, filled=True,\n",
    "    rounded=True,  \n",
    "    special_characters=True,\n",
    "    proportion=True,\n",
    ")\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Classifiers:\n",
    "Let us compare the performance of other classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "clf_lr=lr.fit(X_train,y_train)\n",
    "\n",
    "lr_train_acc = clf_lr.score(X_train,y_train.ravel())\n",
    "lr_train_acc = str(lr_train_acc.round(2) * 100) + ' %'\n",
    "\n",
    "\n",
    "lr_test_acc = clf_lr.score(X_validation_sub,y_validation_sub.ravel())\n",
    "lr_test_acc = str(lr_test_acc.round(2) * 100) + ' %'\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print('Training Accuracy= '+ lr_train_acc)\n",
    "print('Testing Accuracy= '+ lr_test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf_rf=rf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "rf_train_acc = clf_rf.score(X_train,y_train)\n",
    "rf_train_acc = str(rf_train_acc.round(2) * 100) + ' %'\n",
    "\n",
    "\n",
    "rf_test_acc = clf_rf.score(X_validation_sub,y_validation_sub.ravel())\n",
    "rf_test_acc = str(rf_test_acc.round(2) * 100) + ' %'\n",
    "\n",
    "print('Random Forest:')\n",
    "print('Training Accuracy='+ rf_train_acc)\n",
    "print('Testing Accuracy='+ rf_test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div dir = \"rtl\">\n",
    "<ul style = 'list-style-type:square'>\n",
    "<li> End of Document\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "$(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    " });\n",
    "</script>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
